{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6584e03f",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Sentiment Analysis\n",
    "**Author**: Lakshya Khetan  \n",
    "**Project**: Twitter Sentiment Analysis for Indian Elections\n",
    "\n",
    "This notebook demonstrates text preprocessing using our modular preprocessing system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83bd4a1",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd38307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.preprocessor import TextPreprocessor\n",
    "from utils.config import ConfigManager\n",
    "from utils.logger import setup_logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ef98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_manager = ConfigManager('../config/config.yaml')\n",
    "config = config_manager.get_config()\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger('preprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b88713",
   "metadata": {},
   "source": [
    "## Load Collected Data\n",
    "\n",
    "Load the Twitter data collected in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collected tweets\n",
    "data_file = '../data/collected_tweets.csv'\n",
    "try:\n",
    "    tweets_df = pd.read_csv(data_file)\n",
    "    print(f\"‚úÖ Loaded {len(tweets_df)} tweets from {data_file}\")\n",
    "    print(f\"Columns: {list(tweets_df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found. Please run the data collection notebook first.\")\n",
    "    # Create sample data for demonstration\n",
    "    tweets_df = pd.DataFrame({\n",
    "        'id': range(1, 6),\n",
    "        'text': [\n",
    "            \"Great work by Modi government! #BJP2024 https://example.com\",\n",
    "            \"RT @user: Not happy with current policies... üòû\",\n",
    "            \"Congress has better vision for India's future #Congress2024\",\n",
    "            \"Election results will be interesting! #Democracy #India\",\n",
    "            \"@politician Your policies are affecting common people badly!!!\"\n",
    "        ],\n",
    "        'created_at': pd.date_range('2023-01-01', periods=5),\n",
    "        'user': [f'user_{i}' for i in range(1, 6)]\n",
    "    })\n",
    "    print(\"üìù Using sample data for demonstration\")\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ce5bd",
   "metadata": {},
   "source": [
    "## Initialize Text Preprocessor\n",
    "\n",
    "Create an instance of our text preprocessing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text preprocessor\n",
    "preprocessor = TextPreprocessor(config)\n",
    "\n",
    "print(\"Text Preprocessor initialized successfully!\")\n",
    "print(f\"Configuration settings:\")\n",
    "print(f\"  - Remove URLs: {config['preprocessing']['text_cleaning']['remove_urls']}\")\n",
    "print(f\"  - Remove mentions: {config['preprocessing']['text_cleaning']['remove_mentions']}\")\n",
    "print(f\"  - Convert to lowercase: {config['preprocessing']['text_cleaning']['convert_lowercase']}\")\n",
    "print(f\"  - Remove stopwords: {config['preprocessing']['text_cleaning']['remove_stopwords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae7055",
   "metadata": {},
   "source": [
    "## Text Cleaning Demonstration\n",
    "\n",
    "Let's see how our preprocessing works on individual tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate text cleaning on sample tweets\n",
    "print(\"Text Cleaning Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, row in tweets_df.head(3).iterrows():\n",
    "    original_text = row['text']\n",
    "    clean_text = preprocessor.clean_text(original_text)\n",
    "    \n",
    "    print(f\"\\n{i+1}. Original:\")\n",
    "    print(f\"   {original_text}\")\n",
    "    print(f\"   Cleaned:\")\n",
    "    print(f\"   {clean_text}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e840048",
   "metadata": {},
   "source": [
    "## Batch Preprocessing\n",
    "\n",
    "Process the entire dataset using our batch preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire dataset\n",
    "print(\"Processing entire dataset...\")\n",
    "processed_df = preprocessor.preprocess_dataframe(\n",
    "    tweets_df, \n",
    "    text_column='text'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Processed {len(processed_df)} tweets\")\n",
    "print(f\"New columns: {list(processed_df.columns)}\")\n",
    "\n",
    "# Display results\n",
    "processed_df[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0e585",
   "metadata": {},
   "source": [
    "## Tokenization and Sequence Creation\n",
    "\n",
    "Convert cleaned text to numerical sequences for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer on cleaned text\n",
    "print(\"Fitting tokenizer...\")\n",
    "clean_texts = processed_df['clean_text'].tolist()\n",
    "preprocessor.fit_tokenizer(clean_texts)\n",
    "\n",
    "# Get vocabulary information\n",
    "vocab_size = preprocessor.get_vocabulary_size()\n",
    "word_index = preprocessor.get_word_index()\n",
    "\n",
    "print(f\"‚úÖ Tokenizer fitted successfully\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample word indices: {dict(list(word_index.items())[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from text\n",
    "sequences = preprocessor.create_sequences_from_dataframe(\n",
    "    processed_df,\n",
    "    text_column='clean_text'\n",
    ")\n",
    "\n",
    "print(f\"Created sequences with shape: {sequences.shape}\")\n",
    "print(f\"Sequence length: {sequences.shape[1]}\")\n",
    "print(f\"\\nSample sequences:\")\n",
    "for i in range(min(3, len(sequences))):\n",
    "    print(f\"{i+1}. {sequences[i][:10]}... (showing first 10 tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581d9a2",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "Analyze the preprocessed data to understand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "text_lengths = processed_df['clean_text'].str.len()\n",
    "word_counts = processed_df['clean_text'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(f\"Average character length: {text_lengths.mean():.1f}\")\n",
    "print(f\"Average word count: {word_counts.mean():.1f}\")\n",
    "print(f\"Max character length: {text_lengths.max()}\")\n",
    "print(f\"Max word count: {word_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0].hist(text_lengths, bins=20, alpha=0.7, color='blue')\n",
    "axes[0].set_title('Character Length Distribution')\n",
    "axes[0].set_xlabel('Characters')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(word_counts, bins=20, alpha=0.7, color='green')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bfd17",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data\n",
    "\n",
    "Save the preprocessed data and fitted tokenizer for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed dataframe\n",
    "processed_file = '../data/preprocessed_tweets.csv'\n",
    "processed_df.to_csv(processed_file, index=False)\n",
    "print(f\"‚úÖ Preprocessed data saved to {processed_file}\")\n",
    "\n",
    "# Save sequences as numpy array\n",
    "sequences_file = '../data/tweet_sequences.npy'\n",
    "np.save(sequences_file, sequences)\n",
    "print(f\"‚úÖ Sequences saved to {sequences_file}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer_file = '../models/tokenizer.pickle'\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "success = preprocessor.save_tokenizer(tokenizer_file)\n",
    "if success:\n",
    "    print(f\"‚úÖ Tokenizer saved to {tokenizer_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to save tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198192c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. ‚úÖ **Loaded raw Twitter data** from the collection phase\n",
    "2. ‚úÖ **Applied text cleaning** (removed URLs, mentions, special characters)\n",
    "3. ‚úÖ **Tokenized text** and created vocabulary\n",
    "4. ‚úÖ **Generated numerical sequences** for model input\n",
    "5. ‚úÖ **Analyzed text statistics** and distributions\n",
    "6. ‚úÖ **Saved preprocessed data** for model training\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "The preprocessed data is now ready for model training. The next notebook will demonstrate:\n",
    "\n",
    "1. **Model Architecture** - Building LSTM sentiment analysis models\n",
    "2. **Training Process** - Training models on the preprocessed data\n",
    "3. **Model Evaluation** - Assessing model performance\n",
    "\n",
    "Navigate to `04_model_training.ipynb` to continue the workflow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
